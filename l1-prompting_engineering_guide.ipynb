{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Guide\n",
    "\n",
    "[Prompt engineering](https://chrisdaily.medium.com/what-is-prompt-engineering-a-comprehensive-guide-4a5d0b4a2b33) is a practice used to design inputs for generative AI tools which results in optimal outputs. It helps refine prompts that are inputted into an artificial intelligence (AI) service to generate text or images and allows developers to have more control over user interactions with the AI.\n",
    "\n",
    "**Sources**\n",
    "\n",
    "* https://medium.com/ai-frontier-x/the-ultimate-mini-guide-to-prompt-engineering-and-how-anyone-can-master-it-bc2351ec0251\n",
    "\n",
    "*  https://medium.com/@researchgraph/prompt-engineering-21112dbfc789\n",
    "\n",
    "* https://medium.com/@fareedkhandev/prompt-engineering-complete-guide-2968776f0431\n",
    "\n",
    "* https://www.promptingguide.ai/techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To know about LLM\n",
    "\n",
    "![LLM_types](resources/2type_llm.png)\n",
    "\n",
    "For most practical applications we would use **Instruction Tuned LLM** instead **Base LMM**  because they have been trained to follow instruction, using a base LLM trained with a huge amount of text data and further fine-tune it with inputs and outputs of human feedback. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An ideal prompt can be broken down to 6 pieces.\n",
    "\n",
    "![PROMPT PARTS](resources/prompt_parts.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **“Task/Instruction”** is the actual prompt, but without the constraints.\n",
    "\n",
    "2. **“Context”** is the actual detailed content and it is the 2nd most important and essential part of your prompt. \n",
    "\n",
    "    *Important*: It is best if the context is concise because when it becomes long and complicated, the results would tend to be not precise and give the desired results as AI would try to focus on a few things.\n",
    "\n",
    "3. **“Examples”** is also an important part of proompting since giving clear and concise examples tells the large language model about the desired outcome and style.\n",
    "\n",
    "4. **“Persona”** is the moment when you say: “Write Like You Are” or “Create in the style of” to create the content as if that person is writing it.\n",
    "\n",
    "5. **“Format” **is about in what shape or form you want your results to be. It can be a table, code, email, blog post, json, etc.\n",
    "\n",
    "6. **“Tone”** is when you say: professional, friendly, casual, concise, emotional, etc. It is the feeling that you want to convey to your readers.\n",
    "\n",
    "![Example](resources/prompt_example.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering Techniques \n",
    "https://www.promptingguide.ai/techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Zero-shot Prompting\n",
    "\n",
    "In zero-shot prompting, models are capable of executing tasks for which they have received no explicit training. The task is specified solely through the prompt, and no labeled examples are provided.\n",
    "However, when zero-shot prompting doesn’t work, few-shot prompting is used.Example: Amodel can be given a prompt like “Translate the following English text to French: ‘Hello, how are you?’” Without being explicitly trained for translation, the model can generate the French translation, even though it hasn’t seen this exact task before.\n",
    "\n",
    "**Example:** A model can be given a prompt like :\n",
    "```\n",
    "*“Translate the following English text to French: ‘Hello, how are you?’”* \n",
    "```\n",
    "    \n",
    "Without being explicitly trained for translation, the model can generate the French translation, even though it hasn’t seen this exact task before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Few-Shot Prompting\n",
    "In the case of few-shot prompting, the model is given various demonstrations or examples in the prompt and is expected to learn and then produce the output through that.\n",
    "\n",
    "**Example:** \n",
    "\n",
    "```\n",
    "TEXT: Today the weather is fantastic.\n",
    "CLASSIFICATION: Positive\n",
    "TEXT: The forniture is samll\n",
    "CLASSIFICATION: Neural\n",
    "TEXT: I don't like your attitude\n",
    "CLASSIFICATION: Negative\n",
    "TEXT: That shot selection was awfull\n",
    "CLASSIFICATION: \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Chain-of-Thought (CoT) Prompting\n",
    "It enables the LLM to provide the user with complex reasoning through intermediate steps. By leading the model through a logical sequence of prompts, we can guarantee responses that consider context, thereby enhancing the overall quality of the generated text.  \n",
    "\n",
    "This prompting technique uses the let’s think step-by-step technique of an LLM and then generates a demonstration for the answer.\n",
    "\n",
    "**Example**\n",
    "![chain of thought](resources/chain%20od%20thought.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Self-consistency prompting\n",
    "\n",
    "Self-consistency aims *\"to replace the naive greedy decoding used in chain-of-thought prompting\"*. The idea is to sample multiple, diverse reasoning paths through **few-shot CoT**, and use the generations to select the most consistent answer. This helps to boost the performance of CoT prompting on tasks involving arithmetic and commonsense reasoning.\n",
    "\n",
    "**Example: \n",
    "**\n",
    "```\n",
    "Standard Prompt: \n",
    "\n",
    "Q:When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "\n",
    "```\n",
    "The response was \"A: 35\", it is wrong.\n",
    "```\n",
    "Self-Consistency Prompt: \n",
    "\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent $15. She has 23 - 15 = 8. The answer is $8 left.\n",
    "\n",
    "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "A:\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Generated Knowledge Prompting\n",
    "In this prompting technique, the user **asks the LLM** to generate **some knowledge** about a particular topic which is then used to generate further content. For example: if you have to write a blog about Cricket. Before asking the LLM to write a blog you first ask the LLM to state some facts about Cricket. The knowledge generated by the LLM can be used to help it in writing a better blog.\n",
    "\n",
    "![diagram of generated Knowledge](resources/diagam_generating_knowledge.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6- Prompt Chaining\n",
    "In this technique, a prompt is broken into a set of smaller sub-prompts. The **response generated is fed** along with the next prompt. It helps in breaking a big problem into smaller sub-problems which the LLM might struggle to address.\n",
    "\n",
    "One common use case of  this techniques involves answering questions about a large text document. It helps if you design two different prompts where the **first** prompt is responsible for extracting relevant quotes to answer a question and a second prompt takes as input the quotes and original document to answer a given question. In other words, you will be creating two different prompts to perform the task of answering a question given in a document.\n",
    "\n",
    "![diagrama](resources/diagam_chain_prompt.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
